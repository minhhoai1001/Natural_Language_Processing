{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd095c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Bidirectional Encoder Representations from Transformers\n",
    "# 1. Giới thiệu chung"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1. Một số khái niệm\n",
    "Trước khi đi vào bài này, chúng ta cần hiểu rõ một số khái niệm:\n",
    "\n",
    "- **Nhiệm vụ phía sau (Downstream task**): Là những tác vụ supervised-learning được cải thiện dựa trên những pretrained model. VD: Chúng ta sử dụng lại các biểu diễn từ học được từ những pretrained model trên bộ văn bản lớn vào một tác vụ phân tích cảm xúc huấn luyện trên bộ văn bản có **kích thước nhỏ hơn**. Áp dụng pretrain-embedding đã giúp cải thiện mô hình. Như vậy tác vụ sử dụng pretrain-embedding được gọi là downstream task.\n",
    "\n",
    "- **Điểm khái quát đánh giá mức độ hiểu ngôn ngữ (GLUE score benchmark)**: GLUE score benchmark là một tập hợp các chỉ số được xây dựng để đánh giá khái quát mức độ hiểu ngôn ngữ của các model NLP. Các đánh giá được thực hiện trên các bộ dữ liệu tiêu chuẩn được qui định tại các convention về phát triển và thúc đẩy NLP. Mỗi bộ dữ liệu tương ứng với một loại tác NLP vụ như: Phân tích cảm xúc (**Sentiment Analysis**), hỏi đáp (**Question and Answering**), dự báo câu tiếp theo (**NSP - Next Sentence Prediction**), nhận diện thực thể trong câu (**NER - Name Entity Recognition**), suy luận ngôn ngữ tự nhiên (**NLI - Natural Languague Inference**).\n",
    "\n",
    "- **Quan hệ văn bản (Textual Entailment)**: Là tác vụ đánh giá mối quan hệ định hướng giữa 2 văn bản? Nhãn output của các cặp câu được chia thành đối lập (**contradiction**), trung lập (**neutral**) hay có quan hệ đi kèm (**textual entailment**). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Cụ thể hơn, chúng ta có các câu:\n",
    "\n",
    "        A: Hôm nay trời mưa.\n",
    "\n",
    "        B: Tôi mang ô tới trường.\n",
    "\n",
    "        C: Hôm nay trời không mưa.\n",
    "\n",
    "        D: Hôm nay là thứ 3.\n",
    "\n",
    "Khi đó (A, B) có mối quan hệ đi kèm. Các cặp câu (A, C) có mối quan hệ đổi lập và (A, D) là trung lập.\n",
    "\n",
    "- **Suy luận ngôn ngữ (Natural Language Inference)**: Là các tác vụ suy luận ngôn ngữ đánh giá mối quan hệ giữa các cặp câu, cũng tương tự như Textual Entailment.\n",
    "\n",
    "- **Phân tích cảm xúc (Sentiment Analysis)**: Phân loại cảm xúc văn bản thành 2 nhãn tích cực (positive) và tiêu cực (negative). Thường được sử dụng trong các hệ thống đánh giá bình luận của người dùng.\n",
    "\n",
    "- **Hỏi đáp (Question and Answering)**: Là thuật toán hỏi và đáp. Đầu vào là một cặp câu (pair sequence) bao gồm: câu hỏi (question) có chức năng hỏi và đoạn văn bản (paragraph) chứa thông tin trả lời cho câu hỏi. Một bộ dữ liệu chuẩn nằm trong GLUE dataset được sử dụng để đánh giá tác vụ hỏi và đáp là [SQuAD - Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/). Đây là một bài toán khá thú vị, các bạn có thể xem thêm ứng dụng Question and Answering - BERT model mà mình đã sharing.\n",
    "\n",
    "- **Ngữ cảnh (Contextual)**: Là ngữ cảnh của từ. Một từ được định nghĩa bởi một cách phát âm nhưng khi được đặt trong những câu khác nhau thì có thể mang ngữ nghĩa khác nhau. ngữ cảnh có thể coi là môi trường xung quanh từ để góp phần định nghĩa từ. VD:\n",
    "\n",
    "        A: Tôi `đồng` ý với ý kiến của anh.\n",
    "\n",
    "        B: Lão Hạc phải kiếm từng `đồng` để nuôi cậu Vàng.\n",
    "\n",
    "Thì từ `đồng` trong câu A và B có ý nghĩa khác nhau. Chúng ta biết điều này vì dựa vào ngữ cảnh của từ."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.2. Ngữ cảnh (Contextual) và vai trò trong NLP\n",
    "Trước khi tìm hiểu các kỹ thuật đã tạo ra ưu thế vượt trội cho mô hình BERT. Chúng ta hãy khám phá vai trò của ngữ cảnh trong NLP.\n",
    "\n",
    "Bản chất của ngôn ngữ là âm thanh được phát ra để diễn giải dòng suy nghĩ của con người. Trong giao tiếp, các từ thường không đứng độc lập mà chúng sẽ đi kèm với các từ khác để liên kết mạch lạc thành một câu. Hiệu quả biểu thị nội dung và truyền đạt ý nghĩa sẽ lớn hơn so với từng từ đứng độc lập.\n",
    "\n",
    "Ngữ cảnh trong câu có một sự ảnh hưởng rất lớn trong việc giải thích ý nghĩa của từ. Hiểu được vai trò mấu chốt đó, các thuật toán NLP SOTA đều cố gắng đưa ngữ cảnh vào mô hình nhằm tạo ra sự đột phá và cải tiến và mô hình BERT cũng như vậy.\n",
    "\n",
    "Phân cấp mức độ phát triển của các phương pháp embedding từ trong NLP có thể bao gồm các nhóm:\n",
    "\n",
    "**Non-context (không bối cảnh)**: Là các thuật toán không tồn tại bối cảnh trong biểu diễn từ. Đó là các thuật toán NLP đời đầu như **word2vec**, **GLoVe**, **fasttext**. Chúng ta chỉ có duy nhất một biểu diễn véc tơ cho mỗi một từ mà không thay đổi theo bối cảnh. VD:\n",
    "\n",
    "Câu A: Đơn vị tiền tệ của Việt Nam là `đồng`\n",
    "\n",
    "Câu B: Vợ `đồng` ý với ý kiến của chồng là tăng thêm mỗi tháng 500k tiền tiêu vặt\n",
    "\n",
    "Thì từ đồng sẽ mang 2 ý nghĩa khác nhau nên phải có hai biểu diễn từ riêng biệt. Các thuật toán non-context đã không đáp ứng được sự đa dạng về ngữ nghĩa của từ trong NLP.\n",
    "\n",
    "**Uni-directional (một chiều)**: Là các thuật toán đã bắt đầu xuất hiện bối cảnh của từ. Các phương pháp nhúng từ base trên RNN là những phương pháp nhúng từ một chiều. Các kết quả biểu diễn từ đã có bối cảnh nhưng chỉ được giải thích bởi một chiều từ trái qua phải hoặc từ phải qua trái. VD:\n",
    "\n",
    "Câu C: Hôm nay tôi mang 200 tỷ `gửi` ở ngân hàng.\n",
    "\n",
    "Câu D: Hôm nay tôi mang 200 tỷ `gửi` ….\n",
    "\n",
    "Như vậy véc tơ biểu diễn của từ gửi được xác định thông qua các từ liền trước với nó. Nếu chỉ dựa vào các từ liền trước Hôm nay tôi mang 200 tỷ thì ta có thể nghĩ từ phù hợp ở vị trí hiện tại là cho `vay`, `mua`, `thanh toán`,....\n",
    "\n",
    "Ví dụ đơn giản trên đã cho thấy các thuật toán biểu diễn từ có bối cảnh tuân theo theo một chiều sẽ gặp hạn chế lớn trong biểu diễn từ hơn so với biểu diễn 2 chiều.\n",
    "\n",
    "ELMo là một ví dụ cho phương pháp một chiều. Mặc dù ELMo có kiến trúc dựa trên một mạng BiLSTM xem xét bối cảnh theo hai chiều từ trái sang phải và từ phải sang trái nhưng những chiều này là độc lập nhau nên ta coi như đó là biểu diễn một chiều.\n",
    "\n",
    "Thuật toán ELMo đã cải tiến hơn so với word2vec và fasttext đó là tạo ra nghĩa của từ theo bối cảnh. Trong ví dụ về từ đồng thì ở mỗi câu A và B chúng ta sẽ có một biểu diễn từ khác biệt.\n",
    "\n",
    "**Bi-directional (hai chiều)**: Ngữ nghĩa của một từ không chỉ được biểu diễn bởi những từ liền trước mà còn được giải thích bởi toàn bộ các từ xung quanh. Luồng giải thích tuân theo đồng thời từ trái qua phải và từ phải qua trái cùng một lúc. Đại diện cho các phép biểu diễn từ này là những mô hình sử dụng kỹ thuật `transformer` mà chúng ta sẽ tìm hiểu bên dưới. Gần đây, những thuật toán NLP theo trường phái bidirectional như BERT, ULMFit, OpenAI GPT đã đạt được những kết quả SOTA trên hầu hết các tác vụ của `GLUE benchmark`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2. Giới thiệu về BERT\n",
    "BERT là viết tắt của cụm từ `Bidirectional Encoder Representation from Transformer` có nghĩa là mô hình biểu diễn từ theo 2 chiều ứng dụng kỹ thuật Transformer. BERT được thiết kế để huấn luyện trước các biểu diễn từ (pre-train word embedding). Điểm đặc biệt ở BERT đó là nó có thể điều hòa cân bằng bối cảnh theo cả 2 chiều trái và phải.\n",
    "\n",
    "Cơ chế attention của Transformer sẽ truyền toàn bộ các từ trong câu văn đồng thời vào mô hình một lúc mà không cần quan tâm đến chiều của câu. Do đó Transformer được xem như là huấn luyện hai chiều (**bidirectional**) mặc dù trên thực tế chính xác hơn chúng ta có thể nói rằng đó là huấn luyện không chiều (**non-directional**). Đặc điểm này cho phép mô hình học được bối cảnh của từ dựa trên toàn bộ các từ xung quanh nó bao gồm cả từ bên trái và từ bên phải."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1. Fine-tuning model BERT\n",
    "Một điểm đặc biệt ở BERT mà các model embedding trước đây chưa từng có đó là kết quả huấn luyện có thể fine-tuning được. Chúng ta sẽ thêm vào kiến trúc model một output layer để tùy biến theo tác vụ huấn luyện.\n",
    "\n",
    "![](images/pic4.png)\n",
    "\n",
    "Toàn bộ tiến trình pre-training và fine-tuning của BERT. Một kiến trúc tương tự được sử dụng cho cả pretrain-model và fine-tuning model. Chúng ta sử dụng cùng một tham số pretrain để khởi tạo mô hình cho các tác vụ down stream khác nhau. Trong suốt quá trình fine-tuning thì toàn bộ các tham số của layers học chuyển giao sẽ được fine-tune. Đối với các tác vụ sử dụng input là một cặp sequence (pair-sequence) ví dụ như **question and answering** thì ta sẽ thêm token khởi tạo là `[CLS]` ở đầu câu, token `[SEP]` ở giữa để ngăn cách 2 câu."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Tiến trình áp dụng fine-tuning sẽ như sau:\n",
    "\n",
    "- **Bước 1**: Embedding toàn bộ các token của cặp câu bằng các véc tơ nhúng từ pretrain model. Các token embedding bao gồm cả 2 token là `[CLS]` và `[SEP]` để đánh dấu vị trí bắt đầu của câu hỏi và vị trí ngăn cách giữa 2 câu. 2 token này sẽ được dự báo ở output để xác định các phần `Start/End Spand` của câu output.\n",
    "\n",
    "- **Bước 2**: Các embedding véc tơ sau đó sẽ được truyền vào kiến trúc `multi-head attention` với nhiều block code (thường là 6, 12 hoặc 24 blocks tùy theo kiến trúc BERT). Ta thu được một véc tơ output ở encoder.\n",
    "\n",
    "- **Bước 3**: Để dự báo phân phối xác suất cho từng vị trí từ ở decoder, ở mỗi time step chúng ta sẽ truyền vào decoder véc tơ output của encoder và véc tơ embedding input của decoder để tính encoder-decoder attention (cụ thể về encoder-decoder attention là gì các bạn xem lại mục 2.1.1). Sau đó projection qua liner layer và softmax để thu được phân phối xác suất cho output tương ứng ở time step $t$.\n",
    "\n",
    "- **Bước 4**: Trong kết quả trả ra ở output của transformer ta sẽ cố định kết quả của câu Question sao cho trùng với câu Question ở input. Các vị trí còn lại sẽ là thành phần mở rộng `Start/End Span` tương ứng với câu trả lời tìm được từ câu input.\n",
    "\n",
    "Lưu ý quá trình huấn luyện chúng ta sẽ fine-tune lại toàn bộ các tham số của model BERT đã cut off top linear layer và huấn luyện lại từ đầu các tham số của linear layer mà chúng ta thêm vào kiến trúc model BERT để customize lại phù hợp với bài toán.\n",
    "\n",
    "Như vậy các bạn đã hình dung được model BERT được fine-tuning trong một tác vụ như thế nào rồi chứ? Tôi cá rằng qua quá trình thực hành ở bài sau các bạn sẽ nắm vững hơn cách thức fine-tune BERT model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.2. Masked ML (MLM)\n",
    "Masked ML là một tác vụ cho phép chúng ta fine-tuning lại các biểu diễn từ trên các bộ dữ liệu unsupervised-text bất kỳ. Chúng ta có thể áp dụng Masked ML cho những ngôn ngữ khác nhau để tạo ra biểu diễn embedding cho chúng. Các bộ dữ liệu của tiếng anh có kích thước lên tới vài vài trăm tới vài nghìn GB được huấn luyện trên BERT đã tạo ra những kết quả khá ấn tượng.\n",
    "\n",
    "Bên dưới là sơ đồ huấn luyện BERT theo tác vụ Masked ML\n",
    "\n",
    "![](images/pic5.png)\n",
    "\n",
    "Sơ đồ kiến trúc BERT cho tá vụ Masked ML.\n",
    "\n",
    "Theo đó:\n",
    "\n",
    "- Khoảng 15 % các token của câu input được thay thế bởi `[MASK]` token trước khi truyền vào model đại diện cho những từ bị che dấu (masked). Mô hình sẽ dựa trên các từ không được che (non-masked) dấu xung quanh `[MASK]` và đồng thời là bối cảnh của `[MASK]` để dự báo giá trị gốc của từ được che dấu. Số lượng từ được che dấu được lựa chọn là một số ít (15%) để tỷ lệ bối cảnh chiếm nhiều hơn (85%).\n",
    "\n",
    "- Bản chất của kiến trúc BERT vẫn là một mô hình `seq2seq` gồm 2 phase encoder giúp embedding các từ input và decoder giúp tìm ra phân phối xác suất của các từ ở output. Kiến trúc Transfomer encoder được giữ lại trong tác vụ Masked ML. Sau khi thực hiện **self-attention** và **feed forward** ta sẽ thu được các véc tơ embedding ở output là $O_1, O_2,…, O_5$.\n",
    "\n",
    "- Để tính toán phân phối xác suất cho từ output, chúng ta thêm một **Fully connect layer** ngay sau Transformer Encoder. Hàm softmax có tác dụng tính toán phân phối xác suất. Số lượng units của fully connected layer phải bằng với kích thước của từ điển.\n",
    "\n",
    "- Cuối cùng ta thu được véc tơ nhúng của mỗi một từ tại vị trí MASK sẽ là embedding véc tơ giảm chiều của véc tơ $O_i$ sau khi đi qua fully connected layer như mô tả trên hình vẽ bên phải.\n",
    "\n",
    "Hàm loss function của BERT sẽ bỏ qua mất mát từ những từ không bị che dấu và chỉ đưa vào mất mát của những từ bị che dấu. Do đó mô hình sẽ hội tụ lâu hơn nhưng đây là đặc tính bù trừ cho sự gia tăng ý thức về bối cảnh. Việc lựa chọn ngẫu nhiên 15% số lượng các từ bị che dấu cũng tạo ra vô số các kịch bản input cho mô hình huấn luyện nên mô hình sẽ cần phải huấn luyện rất lâu mới học được toàn diện các khả năng."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3. Thực hành model BERT\n",
    "## 3.1. Giới thiệu về bài toán\n",
    "Chúng ta sẽ cùng xây dựng một ứng dụng Question and Answering có chức năng hỏi đáp.\n",
    "\n",
    "Dữ liệu bao gồm:\n",
    "\n",
    "**Inpu**t: Một cặp câu `<Question, Paragraph>`, Question là câu hỏi và Paragraph là đoạn văn bản chứa câu trả lời cho câu hỏi.\n",
    "\n",
    "**Output**: Câu trả lời được trích suất từ Paragraph.\n",
    "\n",
    "Để thực hiện tác vụ này tôi sẽ sử dụng pretrain model từ package transformer. Chúng ta có thể cài thông qua câu lệnh bên dưới.\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.2. Xây dựng một ứng dụng Question and Answering\n",
    "Các bước dữ liệu:\n",
    "\n",
    "- **Tokenize**: Tạo chuỗi token là concatenate của cặp câu <Question, Paragraph>, thêm các token `[CLS]` đánh dấu vị trí bắt đầu câu Question và `[SEP]` đánh dấu vị trí kết thúc câu. Sau đó Tokenize toàn bộ cặp câu <Question, Paragraph> thành chuỗi index từ từ điển.\n",
    "\n",
    "- **Set Segment ID**s: Tạo véc tơ segment cho cặp câu Question và Paragraph. Trong đó index 0 đánh dấu các vị trí thuộc câu A và index 1 đánh dấu các vị trí thuộc câu B.\n",
    "\n",
    "- **Evaluate**: Khởi tạo model từ pretrain model `bert-large-uncased-whole-word-masking-finetuned-squad`. Và dự báo các vị trí start và end nằm trong chuỗi token.\n",
    "\n",
    "- **Reconstruct Answer**: Trích suất thông tin câu trả lời.\n",
    "\n",
    "Source code của mô hình được tham khảo tại [Question answering with fine tuned BERT](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/#part-2-example-code)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForQuestionAnswering\n",
    "import torch\n",
    "# Initialize tokenizer for corpus of bert-large-uncased\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Initialize model BertForQuestionAnswering for bert-large-uncased\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Lấy input là chuỗi string của câu question và answer_text chứa nội dung câu trả lời của câu question.\n",
    "    Xác định từ trong answer_text là câu trả lời và in ra.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Áp dụng tokenizer cho cặp câu <question, answer_text>. input_ids là concatenate indice của cả 2 câu sau khi đã thêm các token CLS và SEP như mô tả trong tác vụ Question and Answering.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Xác định vị trí đầu tiên chứa token [SEP] trong câu.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # Tạo segment index đánh dấu các vị trí từ thuộc question (giá trị 0) và answer_text (giá trị 1)\n",
    "    num_seg_a = sep_index + 1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # Kiểm tra độ dài segment_ids phải bằng input_ids\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Dự báo phân phối xác suất của vị trí của từ start và từ end trong chuỗi concatenate <question, answer_text> mà chứa kết quả cho câu trả lời.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # chuỗi index biểu thị cho inputs.\n",
    "                                    token_type_ids=torch.tensor([segment_ids]), return_dict=False) # chuỗi index thành phần segment câu để phân biệt giữa câu question và câu answer_text\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Tìm ra vị trí start, end với score là cao nhất\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Chuyển ngược từ input_ids sang list tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Token đầu tiên của câu trả lời\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Lựa chọn các thành phần còn lại của câu trả lời và join chúng với whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # Nếu token là một subword token (có dấu ## ở đầu) thì combine vào answer bằng token gốc (loại bỏ dấu ##).\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Nếu trái lại thì combine trực tiếp vào answer.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "    print('Question: \"' + question + '\"')\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "source": [
    "Thử nghiệm kết quả của mô hình trên một vài cặp câu `<Question, Paragraph>`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Question: \"what is my dog name?\"\nAnswer: \"ricky\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what is my dog name?\"\n",
    "paragraph = \"I have a dog. It's name is Ricky. I get it at my 15th birthday, when it was a puppy.\"\n",
    "\n",
    "answer_question(question, paragraph)"
   ]
  },
  {
   "source": [
    "Thử nghiệm một văn bản khác dài hơn. Tôi sẽ lấy một đoạn văn mô tả tiểu sử của ông vua toán học `Euler` và hỏi thuật toán ngày sinh của ông ấy. Các bạn hãy xem kết quả nhé."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Question: \"when Leonhard Euler was born?\"\nAnswer: \"15 april 1707\"\n"
     ]
    }
   ],
   "source": [
    "question = \"when Leonhard Euler was born?\"\n",
    "paragraph = \"Leonhard Euler: 15 April 1707 – 18 September 1783 was a Swiss mathematician, \\\n",
    "physicist, astronomer, geographer, logician and engineer who made important and influential discoveries in many branches of mathematics, \\\n",
    "such as infinitesimal calculus and graph theory, \\\n",
    "while also making pioneering contributions to several branches such as topology and analytic number theory. \\\n",
    "He also introduced much of the modern mathematical terminology and notation, \\\n",
    "particularly for mathematical analysis, such as the notion of a mathematical function.[4] He is also known for his work in mechanics, fluid dynamics, optics, astronomy and music theory\"\n",
    "\n",
    "answer_question(question, paragraph)"
   ]
  },
  {
   "source": [
    "Ta có thể thấy kết quả là chính xác.\n",
    "\n",
    "Việc áp dụng pretrain model sẵn có trên package transformer cho tác vụ **Question and Answering** là khá dễ dàng. Chúng ta cũng có thể fine-tuning lại các kiến trúc model question and answering cho dữ liệu Tiếng Việt để tạo ra các ứng dụng hỏi đáp cho riêng mình. Để thực hiện được điều đó đòi hỏi phải nắm vững kiến trúc của model BERT được trình bày trong bài viết này. Có lẽ ở một bài sau tôi sẽ hướng dẫn các bạn thực hành điều này."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4. Tổng kết\n",
    "Như vậy qua bài này tôi đã hướng dẫn các bạn kiến trúc tổng quát của model BERT và cách thức áp dụng model BERT vào trong các tác vụ down stream task trong NLP như Masked ML, Next Sentence Prediction và thực hành xây dựng một ứng dụng Question and Answering ngay trên pretrain model của transformer package.\n",
    "\n",
    "Các kiến trúc biến thể mới của BERT hiện tại vẫn đang được nghiên cứu và tiếp tục phát triển như ROBERTA, ALBERT, CAMEBERT, XLMROBERTA, …\n",
    "\n",
    "Ngày càng có nhiều các pretrain model trên BERT áp dụng cho nhiều ngôn ngữ khác nhau trên toàn thế giới và tạo ra một sự đột phá trong NLP. Ngôn ngữ Tiếng Việt của chúng ta cũng đã được VinAI nghiên cứu và huấn luyện pretrain model thành công. Bạn đọc muốn sử dụng pretrain model này trong các tác vụ NLP có thể tham khảo thêm tại PhoBERT."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 5. Tài liệu\n",
    "1. [From word embeddings to Pretrained Language models](https://towardsdatascience.com/from-word-embeddings-to-pretrained-language-models-a-new-age-in-nlp-part-2-e9af9a0bdcd9)\n",
    "\n",
    "2. [BERT explained state of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
    "\n",
    "3. [Huggingface - transformer github package](https://github.com/huggingface/transformers/)\n",
    "\n",
    "4. [Question answering with a fine tuned BERT](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/)\n",
    "\n",
    "5. [BERT fine-tuning with cloud](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "\n",
    "6. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "\n",
    "7. [OpenAI GPT - paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "8. [ULMFit paper](https://arxiv.org/abs/1801.06146)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 6. Nguồn\n",
    "[BERT model - Pham Dinh Khanh](https://phamdinhkhanh.github.io/2020/05/23/BERTModel.html#6-t%C3%A0i-li%E1%BB%87u)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}