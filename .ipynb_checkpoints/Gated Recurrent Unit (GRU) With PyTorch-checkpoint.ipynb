{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU) With PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Have you heard of GRUs?*\n",
    "\n",
    "The Gated Recurrent Unit (GRU) is the younger sibling of the more popular [Long Short-Term Memory (LSTM) network](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/), and also a type of [Recurrent Neural Network (RNN)](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/). Just like its sibling, GRUs are able to effectively retain long-term dependencies in sequential data. And additionally, they can address the “short-term memory” issue plaguing vanilla RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the legacy of Recurrent architectures in sequence modelling and predictions, the GRU is on track to outshine its elder sibling due to its superior speed while achieving similar accuracy and effectiveness.\n",
    "\n",
    "In this article, we’ll walk through the concepts behind GRUs and compare the mechanisms of GRUs against LSTMs. We’ll also explore the performance differences in these two RNN variants. If you are unfamiliar with RNNs or LSTMs, you can have a look through my previous posts covering those topics:\n",
    "\n",
    "- [A Beginner’s Guide on Recurrent Neural Networks](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)\n",
    "- [Long Short-Term Memory: From Zero to Hero](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What are GRUs?\n",
    "A Gated Recurrent Unit (GRU), as its name suggests, is a variant of the RNN architecture, and uses gating mechanisms to control and manage the flow of information between cells in the neural network. GRUs were introduced only in 2014 by [Cho, et al](https://arxiv.org/pdf/1406.1078.pdf). and can be considered a relatively new architecture, especially when compared to the widely-adopted LSTM, which was proposed in 1997 by [Sepp Hochreiter and Jürgen Schmidhuber](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory).\n",
    "\n",
    "![](images/image17-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the GRU allows it to adaptively capture dependencies from large sequences of data without discarding information from earlier parts of the sequence. This is achieved through its **gating** units, similar to the ones in LSTMs, which solve the vanishing/exploding gradient problem of traditional RNNs. These gates are responsible for regulating the information to be kept or discarded at each time step. We’ll dive into the specifics of how these gates work and how they overcome the above issues later in this article.\n",
    "\n",
    "![](images/image15.jpg)\n",
    "*GRUs follow the same flow as the typical RNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than its internal gating mechanisms, the GRU functions just like an RNN, where sequential input data is consumed by the GRU cell at each time step along with the memory, or otherwise known as the **hidden state**. The hidden state is then re-fed into the RNN cell together with the next input data in the sequence. This process continues like a relay system, producing the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. But How Does It Really Work? Inner Workings of the GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability of the GRU to hold on to long-term dependencies or memory stems from the computations within the GRU cell to produce the hidden state. While **LSTMs** have two different states passed between the cells — **the cell** state and **hidden state**, which carry the long and short-term memory, respectively — GRUs only have one hidden state transferred between time steps. This hidden state is able to hold both the long-term and short-term dependencies at the same time due to the gating mechanisms and computations that the hidden state and input data go through.\n",
    "\n",
    "![](images/image11.jpg)\n",
    "\n",
    "*GRU vs LSTM*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU cell contains only two gates: the **Update gate** and the **Reset gate**. Just like the gates in LSTMs, these gates in the GRU are **trained** to selectively filter out any irrelevant information while keeping what’s useful. These gates are essentially vectors containing values between $0$ to $1$ which will be multiplied with the input data and/or hidden state. A $0$ value in the gate vectors indicates that the corresponding data in the input or hidden state is unimportant and will, therefore, return as a zero. On the other hand, a $1$ value in the gate vector means that the corresponding data is important and will be used.\n",
    "\n",
    "I’ll be using the terms **gate** and **vector** interchangeably for the rest of this article, as they refer to the same thing.\n",
    "\n",
    "The structure of a GRU unit is shown below.\n",
    "\n",
    "![](images/image14.jpg)\n",
    "*Inner workings of the GRU cell*\n",
    "\n",
    "While the structure may look rather complicated due to the large number of connections, the mechanism behind it can be broken down into three main steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Reset Gate\n",
    "In the first step, we’ll be creating the Reset gate. This gate is derived and calculated using both the hidden state from the previous time step and the input data at the current time step.\n",
    "\n",
    "![](images/image9.jpg)\n",
    "\n",
    "*Reset Gate Flow*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, this is achieved by multiplying the **previous hidden state** and **current input** with their respective weights and summing them before passing the sum through a **sigmoid** function. The **sigmoid** function will transform the values to fall between 0 and 1, allowing the gate to filter between the less-important and more-important information in the subsequent steps.\n",
    "\n",
    "<center>$gate_{reset} = \\sigma(W_{input_{reset}} \\cdot x_t + W_{hidden_{reset}} \\cdot h_{t-1})$</center>\n",
    "\n",
    "When the entire network is trained through back-propagation, the **weights** in the equation will be updated such that the **vector** will learn to retain only the useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **previous hidden state** will first be multiplied by a trainable weight and will then undergo an element-wise multiplication (Hadamard product) with the **reset vector**. This operation will decide which information is to be kept from the previous time steps together with the new inputs. At the same time, the **current input** will also be multiplied by a trainable weight before being summed with the product of the **reset vector** and **previous hidden state** above. Lastly, a non-linear activation tanh function will be applied to the final result to obtain **r** in the equation below.\n",
    "\n",
    "\n",
    "<center>$r = tanh(gate_{reset} \\odot (W_{h_1} \\cdot h_{t-1}) + W_{x_1} \\cdot x_t)$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Update Gate\n",
    "Next, we’ll have to create the **Update gate**. Just like the **Reset gate**, the gate is computed using the previous hidden state and current input data.\n",
    "\n",
    "![](images/image13.jpg)\n",
    "*Update Gate Flow*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the **Update** and **Reset** gate vectors are created using the same formula, but, the weights multiplied with the input and hidden state are unique to each gate, which means that  the final vectors for each gate are different. This allows the gates to serve their specific purposes.\n",
    "\n",
    "<center>$(images/gate_{update} = \\sigma(W_{input_{update}} \\cdot x_t + W_{hidden_{update}} \\cdot h_{t-1})$</center>\n",
    "\n",
    "The **Update** vector will then undergo element-wise multiplication with the **previous hidden state** to obtain **u** in our equation below, which will be used to compute our final output later.\n",
    "\n",
    "<center>$u = gate_{update} \\odot h_{t-1}$</center>\n",
    "\n",
    "The **Update** vector will also be used in another operation later when obtaining our final output. The purpose of the **Update** gate here is to help the model determine how much of the past information stored in the **previous hidden state** needs to be retained for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Combining the outputs\n",
    "In the last step, we will be reusing the **Update** gate and obtaining the **updated hidden state**.\n",
    "\n",
    "![](images/image8.jpg)\n",
    "\n",
    "*Final Output Computations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will be taking the element-wise inverse version of the same **Update** vector ($1 - update gate$) and doing an element-wise multiplication with our output from the **Reset gate**, **r**. The purpose of this operation is for the **Update gate** to determine what portion of the new information should be stored in the **hidden state**.\n",
    "\n",
    "Lastly, the result from the above operations will be summed with our output from the **Update gate** in the previous step, **u**. This will give us our new and **updated hidden state**.\n",
    "\n",
    "<center>$h_t = r \\odot (1-gate_{update}) + u$</center>\n",
    "\n",
    "We can use this **new** hidden state as our output for that time step as well by passing it through a linear activation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Solving the Vanishing / Exploding Gradient Problem\n",
    "\n",
    "We’ve seen the gates in action. We know how they transform our data. Now let’s review their overall role in managing the network’s memory and talk about how they solve the vanishing/exploding gradient problem.\n",
    "\n",
    "As we’ve seen in the mechanisms above, the **Reset gate** is responsible for deciding which portions of the **previous hidden state** are to be combined with the **current input** to **propose** a new hidden state.\n",
    "\n",
    "And the **Update gate** is responsible for determining how much of the **previous hidden state** is to be retained and what portion of the new proposed hidden state (derived from the **Reset gate**) is to be added to the **final hidden state**. When the **Update gate** is first multiplied with the previous hidden state, the network is picking which parts of the previous hidden state it is going to keep in its memory while discarding the rest. Subsequently, it is patching up the **missing parts** of information when it uses the inverse of the **Update gate** to filter the proposed new hidden state from the **Reset gate**.\n",
    "\n",
    "This allows the network to retain **long-term dependencies**. The Update gate can choose to retain most of the previous memories in the hidden state if the **Update vector** values are close to 1 without re-computing or changing the entire hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanishing/exploding gradient problem occurs during back-propagation when training the RNN, especially if the RNN is processing long sequences or has multiple layers. The **error gradient** calculated during training is used to update the network’s weight in the right direction and by the right magnitude. However, this gradient is calculated  with the chain rule, starting from the end of the network. Therefore, during back-propagation, the gradients will continuously undergo matrix multiplications and either shrink (vanish) or blow up (explode) **exponentially** for long sequences. Having a gradient that is too small means the model won’t update its weights effectively, whereas extremely large gradients cause the model to be unstable.\n",
    "\n",
    "The gates in the LSTM and GRUs help to solve this problem because of the additive component of the **Update gates**. While traditional RNNs always replace the entire content of the hidden state at each time step, LSTMs and GRUs keep most of the existing hidden state while adding new content on top of it. This allows the error gradients to be back-propagated without vanishing or exploding too quickly due to the addition operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While LSTMs and GRUs are the most widely-used fixes to the above problem, another solution to the problem of exploding gradients is **gradient clipping**. Clipping sets a defined threshold value on the gradients, which means that even if a gradient increases beyond the predefined value during training, its value will still be limited to the set threshold. This way, the direction of the gradient remains unaffected and only the magnitude of the gradient is changed.\n",
    "\n",
    "![](images/image10.png)\n",
    "\n",
    "*Gradient Clipping - Image Reference: Ian Goodfellow et. al, “Deep Learning”, MIT press, 2016*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GRU vs LSTM\n",
    "We’ve got the mechanics of GRUs down. But how does it compare to its older (and more popular) sibling, LSTMs?\n",
    "\n",
    "Well, both  were created to solve the vanishing/exploding gradient problem that the standard RNN faces, and both of these RNN variants utilise gating mechanisms to control the flow of long-term and short-term dependencies within the network.\n",
    "\n",
    "But how are they different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Structural Differences\n",
    "While both GRUs and LSTMs contain gates, the main difference between these two structures lies in the number of gates and their specific roles. The role of the **Update gate** in the GRU is very similar to the **Input** and **Forget gates** in the LSTM. However, the control of **new memory content** added to the network differs between these two.\n",
    "\n",
    "![](images/image12.jpg)\n",
    "\n",
    "*Comparison Of GRU VS LSTM Structure*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the LSTM, while the **Forget gate** determines which part of the previous **cell state** to retain, the **Input gate** determines the amount of **new memory** to be added. These two gates are independent of each other, meaning that the amount of new information added through the **Input gate** is completely independent of the information retained through the **Forget gate**.\n",
    "\n",
    "As for the GRU, the **Update gate** is responsible for determining which information from the previous memory to retain and is also responsible for controlling the new memory to be added. This means that the retention of previous memory and addition of new information to the memory in the GRU is NOT independent.\n",
    "\n",
    "Another key difference between the structures is the lack of the cell state in the GRU, as mentioned earlier. While the LSTM stores its longer-term dependencies in the cell state and short-term memory in the hidden state, the GRU stores both in a single hidden state. However, in terms of effectiveness in retaining long-term information, both architectures have been proven to achieve this goal effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Speed Differences\n",
    "\n",
    "GRUs are faster to train as compared to LSTMs due to the fewer number of weights and parameters to update during training. This can be attributed to the fewer number of gates in the GRU cell (two gates) as compared to the LSTM’s three gates.\n",
    "\n",
    "In the code walkthrough further down in this article, we’ll be directly comparing the speed of training an LSTM against a GRU on the exact same task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Performance Evaluations\n",
    "\n",
    "The accuracy of a model, whether it is measured by the margin of error or proportion of correct classifications, is usually the main factor when deciding which type of model to use for a task. Both GRUs and LSTMs are variants of RNNS and can be plugged in interchangeably to achieve similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Project: Time-series Prediction with GRU and LSTM\n",
    "\n",
    "We’ve learnt about the theoretical concepts behind the GRU. Now it’s time to put that learning to work.\n",
    "\n",
    "We’ll be implementing a GRU model in code. To further our GRU-LSTM comparison, we’ll also be using an LSTM model to complete the same task. We’ll evaluate the performance of both models on a few metrics. The dataset that we will be using is the Hourly Energy Consumption dataset, which can be found on [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). The dataset contains power consumption data across different regions around the United States recorded on an hourly basis.\n",
    "\n",
    "This will speed up the training process significantly. Alternatively, you can [visit the GitHub repository](https://github.com/gabrielloye/GRU_Prediction) specifically.\n",
    "\n",
    "The goal of this implementation is to create a model that can accurately predict the energy usage in the next hour given historical usage data. We will be using both the GRU and LSTM model to train on a set of historical data and evaluate both models on an unseen test set. To do so, we’ll start with feature selection and data pre-processing, followed by defining, training, and eventually evaluating the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the process flow of our project.\n",
    "![](images/image7.jpg)\n",
    "*We will go in detail for each of these steps*\n",
    "\n",
    "We will be using the PyTorch library to implement both types of models along with other common Python libraries used in data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define data root directory\n",
    "data_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>AEP_MW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-12-31 01:00:00</td>\n",
       "      <td>13478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-12-31 02:00:00</td>\n",
       "      <td>12865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-12-31 03:00:00</td>\n",
       "      <td>12577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-12-31 04:00:00</td>\n",
       "      <td>12517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-12-31 05:00:00</td>\n",
       "      <td>12670.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Datetime   AEP_MW\n",
       "0  2004-12-31 01:00:00  13478.0\n",
       "1  2004-12-31 02:00:00  12865.0\n",
       "2  2004-12-31 03:00:00  12577.0\n",
       "3  2004-12-31 04:00:00  12517.0\n",
       "4  2004-12-31 05:00:00  12670.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise how our data looks\n",
    "pd.read_csv(data_dir + 'AEP_hourly.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 12 `.csv` files containing hourly energy trend data of the above format (`'est_hourly.paruqet'` and `'pjm_hourly_est.csv'` are not used). In our next step, we will be reading these files and pre-processing these data in this order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the time data of each individual time step and generalize them\n",
    "    - Hour of the day i.e., `0 - 23`\n",
    "    - Day of the week i.e,. `1 - 7`\n",
    "    - Month i.e., `1 - 12`\n",
    "    - Day of the year i.e., `1 - 365`\n",
    "- Scale the data to values between 0 and 1\n",
    "    - Algorithms tend to perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed\n",
    "    - Scaling preserves the shape of the original distribution and doesn't reduce the importance of outliers\n",
    "- Group the data into sequences to be used as inputs to the model and store their corresponding labels\n",
    "    - The sequence length or look back period is the number of data points in history that the model will use to make the prediction\n",
    "    - The label will be the next data point in time after the last one in the input sequence\n",
    "- Split the inputs and labels into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-463c94a59aa5>:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for file in tqdm_notebook(os.listdir(data_dir)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e877daa0bd49de89f8bdd6f9968f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
    "label_scalers = {}\n",
    "\n",
    "train_x = []\n",
    "test_x = {}\n",
    "test_y = {}\n",
    "\n",
    "for file in tqdm_notebook(os.listdir(data_dir)): \n",
    "    # Skipping the files we're not using\n",
    "    if file[-4:] != \".csv\" or file == \"pjm_hourly_est.csv\":\n",
    "        continue\n",
    "    \n",
    "    # Store csv file in a Pandas DataFrame\n",
    "    df = pd.read_csv('{}/{}'.format(data_dir, file), parse_dates=[0])\n",
    "    # Processing the time data into suitable input formats\n",
    "    df['hour'] = df.apply(lambda x: x['Datetime'].hour,axis=1)\n",
    "    df['dayofweek'] = df.apply(lambda x: x['Datetime'].dayofweek,axis=1)\n",
    "    df['month'] = df.apply(lambda x: x['Datetime'].month,axis=1)\n",
    "    df['dayofyear'] = df.apply(lambda x: x['Datetime'].dayofyear,axis=1)\n",
    "    df = df.sort_values(\"Datetime\").drop(\"Datetime\",axis=1)\n",
    "    \n",
    "    # Scaling the input data\n",
    "    sc = MinMaxScaler()\n",
    "    label_sc = MinMaxScaler()\n",
    "    data = sc.fit_transform(df.values)\n",
    "    # Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation\n",
    "    label_sc.fit(df.iloc[:,0].values.reshape(-1,1))\n",
    "    label_scalers[file] = label_sc\n",
    "    \n",
    "    # Define lookback period and split inputs/labels\n",
    "    lookback = 90\n",
    "    inputs = np.zeros((len(data)-lookback,lookback,df.shape[1]))\n",
    "    labels = np.zeros(len(data)-lookback)\n",
    "    \n",
    "    for i in range(lookback, len(data)):\n",
    "        inputs[i-lookback] = data[i-lookback:i]\n",
    "        labels[i-lookback] = data[i,0]\n",
    "    inputs = inputs.reshape(-1,lookback,df.shape[1])\n",
    "    labels = labels.reshape(-1,1)\n",
    "    \n",
    "    # Split data into train/test portions and combining all data from different files into a single array\n",
    "    test_portion = int(0.1*len(inputs))\n",
    "    if len(train_x) == 0:\n",
    "        train_x = inputs[:-test_portion]\n",
    "        train_y = labels[:-test_portion]\n",
    "    else:\n",
    "        train_x = np.concatenate((train_x,inputs[:-test_portion]))\n",
    "        train_y = np.concatenate((train_y,labels[:-test_portion]))\n",
    "    test_x[file] = (inputs[-test_portion:])\n",
    "    test_y[file] = (labels[-test_portion:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 980,185 sequences of training data.\n",
    "\n",
    "To improve the speed of our training, we can process the data in batches so that the model doesn’t need to update its weights as frequently. The Torch Dataset and DataLoader classes are useful for **splitting** our data into batches and **shuffling** them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if we have any GPUs to speed up our training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll be defining the structure of the GRU and LSTM models. Both models have the same structure, with the only difference being the **recurrent layer** (GRU/LSTM) and the initializing of the hidden state. The hidden state for the LSTM is a tuple containing both the **cell state** and the **hidden state**, whereas the GRU only has a single hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is defined in a function below so that we can reproduce it for both models. Both models will have the same number of **dimensions** in the hidden state and layers, trained over the same number of **epochs** and **learning rate**, and trained and tested on the exact same set of data.\n",
    "\n",
    "For the purpose of comparing the performance of both models, we'll be tracking the time it takes for the model to train and eventually comparing the final accuracy of both models on the test set. For our accuracy measure, we'll use [Symmetric Mean Absolute Percentage Error](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) (sMAPE) to evaluate the models. sMAPE is the sum of the **absolute difference** between the predicted and actual values divided by the average of the predicted and actual value, therefore giving a percentage measuring the amount of error.This is the formula for sMAPE:\n",
    "\n",
    "<center>$sMAPE = \\frac{100%}{n} \\sum_{t=1}^n \\frac{|F_t - A_t|}{(|F_t + A_t|)/2}$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"GRU\"):\n",
    "    \n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.perf_counter()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter%200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.perf_counter()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    for i in test_x.keys():\n",
    "        inp = torch.from_numpy(np.array(test_x[i]))\n",
    "        labs = torch.from_numpy(np.array(test_y[i]))\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1......Step: 200/957....... Average Loss for Epoch: 0.006769843979272991\n",
      "Epoch 1......Step: 400/957....... Average Loss for Epoch: 0.0037734584399004233\n",
      "Epoch 1......Step: 600/957....... Average Loss for Epoch: 0.0026687314940985135\n",
      "Epoch 1......Step: 800/957....... Average Loss for Epoch: 0.002087092123256298\n",
      "Epoch 1/5 Done, Total Loss: 0.0017923857346033559\n",
      "Total Time Elapsed: 196.72291200000018 seconds\n",
      "Epoch 2......Step: 200/957....... Average Loss for Epoch: 0.0002634253797441488\n",
      "Epoch 2......Step: 400/957....... Average Loss for Epoch: 0.0002458331687739701\n",
      "Epoch 2......Step: 600/957....... Average Loss for Epoch: 0.0002350402728431315\n",
      "Epoch 2......Step: 800/957....... Average Loss for Epoch: 0.00022656418566839421\n",
      "Epoch 2/5 Done, Total Loss: 0.00022015131627345241\n",
      "Total Time Elapsed: 198.28586180000002 seconds\n",
      "Epoch 3......Step: 200/957....... Average Loss for Epoch: 0.00018012675747741013\n",
      "Epoch 3......Step: 400/957....... Average Loss for Epoch: 0.00017615445802221074\n",
      "Epoch 3......Step: 600/957....... Average Loss for Epoch: 0.000174727904292619\n",
      "Epoch 3......Step: 800/957....... Average Loss for Epoch: 0.0001690094898094685\n",
      "Epoch 3/5 Done, Total Loss: 0.00016568784606456902\n",
      "Total Time Elapsed: 199.83818820000033 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4e18a4c05c65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgru_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"GRU\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mLstm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"LSTM\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-02af4b777c1f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, learn_rate, hidden_dim, EPOCHS, model_type)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "gru_model = train(train_loader, lr, model_type=\"GRU\")\n",
    "Lstm_model = train(train_loader, lr, model_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the training time of both models, our younger sibling has absolutely thrashed the older one in terms of speed. The GRU model is the clear winner on that dimension; it finished five training epochs 72 seconds faster than the LSTM model.\n",
    "\n",
    "Moving on to measuring the accuracy of both models, we’ll now use our `evaluate()` function and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_outputs, targets, lstm_sMAPE = evaluate(lstm_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh. Interesting, right? While the LSTM model may have made smaller errors and edged in front of the GRU model slightly in terms of performance accuracy, the difference is insignificant and thus inconclusive.\n",
    "\n",
    "Other tests comparing both these models have similarly returned no clear winner as to which is the better architecture overall.\n",
    "\n",
    "Lastly, let's do some visualisations on random sets of our predicted output vs the actual consumption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(gru_outputs[0][-100:], \"-o\", color=\"g\", label=\"Predicted\")\n",
    "plt.plot(targets[0][-100:], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel('Energy Consumption (MW)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(gru_outputs[8][-50:], \"-o\", color=\"g\", label=\"Predicted\")\n",
    "plt.plot(targets[8][-50:], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel('Energy Consumption (MW)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(gru_outputs[4][:50], \"-o\", color=\"g\", label=\"Predicted\")\n",
    "plt.plot(targets[4][:50], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel('Energy Consumption (MW)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(lstm_outputs[6][:100], \"-o\", color=\"g\", label=\"Predicted\")\n",
    "plt.plot(targets[6][:100], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel('Energy Consumption (MW)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the models are largely successful in predicting the trends of energy consumption. While they may still get some changes wrong, such as delays in predicting a drop in consumption, the predictions follow very closely to the actual line on the test set. This is due to the nature of energy consumption data and the fact that there are patterns and cyclical changes that the model can account for. Tougher time-series prediction problems such as stock price prediction or sales volume prediction may have data that is largely random or doesn’t have predictable patterns, and in such cases, the accuracy will definitely be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Beyond GRUs\n",
    "As I mentioned in my [LSTM article](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/), RNNs and their variants have been replaced over the years in various NLP tasks and are no longer an [NLP standard architecture](https://blog.floydhub.com/ten-trends-in-deep-learning-nlp/#2-recurrent-neural-networks-rnns-are-no-longer-an-nlp-standard-architecture). Pre-trained [transformer models](https://blog.floydhub.com/the-transformer-in-pytorch/) such as Google’s BERT, [OpenAI’s GPT](https://blog.floydhub.com/gpt2/) and the recently introduced XLNet have produced state-of-the-art benchmarks and results and have introduced transfer learning for downstreamed tasks to NLP.\n",
    "\n",
    "With that, signing off on all things GRU for now. This piece completes my series of articles covering the basics of RNNs; in future, we’ll be exploring more advanced concepts such as the Attention mechanism, Transformers, and the modern state-of-the-art in NLP. Get hyped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Source\n",
    "[Gated Recurrent Unit (GRU) With PyTorch](https://blog.floydhub.com/gru-with-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
