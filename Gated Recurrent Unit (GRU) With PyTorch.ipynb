{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU) With PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Have you heard of GRUs?*\n",
    "\n",
    "The Gated Recurrent Unit (GRU) is the younger sibling of the more popular [Long Short-Term Memory (LSTM) network](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/), and also a type of [Recurrent Neural Network (RNN)](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/). Just like its sibling, GRUs are able to effectively retain long-term dependencies in sequential data. And additionally, they can address the “short-term memory” issue plaguing vanilla RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the legacy of Recurrent architectures in sequence modelling and predictions, the GRU is on track to outshine its elder sibling due to its superior speed while achieving similar accuracy and effectiveness.\n",
    "\n",
    "In this article, we’ll walk through the concepts behind GRUs and compare the mechanisms of GRUs against LSTMs. We’ll also explore the performance differences in these two RNN variants. If you are unfamiliar with RNNs or LSTMs, you can have a look through my previous posts covering those topics:\n",
    "\n",
    "- [A Beginner’s Guide on Recurrent Neural Networks](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)\n",
    "- [Long Short-Term Memory: From Zero to Hero](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are GRUs?\n",
    "A Gated Recurrent Unit (GRU), as its name suggests, is a variant of the RNN architecture, and uses gating mechanisms to control and manage the flow of information between cells in the neural network. GRUs were introduced only in 2014 by [Cho, et al](https://arxiv.org/pdf/1406.1078.pdf). and can be considered a relatively new architecture, especially when compared to the widely-adopted LSTM, which was proposed in 1997 by [Sepp Hochreiter and Jürgen Schmidhuber](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory).\n",
    "\n",
    "![](images/image17-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the GRU allows it to adaptively capture dependencies from large sequences of data without discarding information from earlier parts of the sequence. This is achieved through its **gating** units, similar to the ones in LSTMs, which solve the vanishing/exploding gradient problem of traditional RNNs. These gates are responsible for regulating the information to be kept or discarded at each time step. We’ll dive into the specifics of how these gates work and how they overcome the above issues later in this article.\n",
    "\n",
    "![](images/image15.jpg)\n",
    "*GRUs follow the same flow as the typical RNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# source\n",
    "[Gated Recurrent Unit (GRU) With PyTorch](https://blog.floydhub.com/gru-with-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
